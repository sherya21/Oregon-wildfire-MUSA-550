[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Oregon Wildfire Prediction",
    "section": "",
    "text": "Wildfires, a phenomenon where an uncontrolled fire that burns in the wildland vegetation, has been a growing concern in the US in recent years. For our final project, we will be using machine learning techniques to explore instances of wildfires across Oregon, in the United States. As wildfires become increasingly common, it is crucial to equip policy-makers and first responders with tools to help better predict areas that are most likely to set ablaze across the State.\n\n\n\n\n\n\nThe National Interagency Fire Center statistics show that as of Dec. 7, 2023, there have been 53,685 fires this year that have burned nearly 2.61 million acres."
  },
  {
    "objectID": "index.html#wildfires",
    "href": "index.html#wildfires",
    "title": "Oregon Wildfire Prediction",
    "section": "",
    "text": "Wildfires, a phenomenon where an uncontrolled fire that burns in the wildland vegetation, has been a growing concern in the US in recent years. For our final project, we will be using machine learning techniques to explore instances of wildfires across Oregon, in the United States. As wildfires become increasingly common, it is crucial to equip policy-makers and first responders with tools to help better predict areas that are most likely to set ablaze across the State.\n\n\n\n\n\n\nThe National Interagency Fire Center statistics show that as of Dec. 7, 2023, there have been 53,685 fires this year that have burned nearly 2.61 million acres."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Oregon Wildfire Prediction",
    "section": "Methodology",
    "text": "Methodology\n\nThe Data:\n\nKaggle fire dataset (1992-2015)\nLand cover (1992-2015)\nClimatic conditions data (pulled from Copernicus Climate Change Service API)\n\n\n\nKey Questions:\n\nHow frequently are wildfires occurring across Oregon?\nWhat factors most contribute to the start and spread of wildfires in Oregon?\nWhich areas are the most susceptible to future wildfires?\nWhat areas have regenerated fuel to burn again in the past and are susceptible to burning in the future?\n\n\n\nAnalytical Techniques\n\nExploratory analysis (temporal + spatial)\nTensorflow and scikit-learn packages to develop regressions for neural network (ANN)\nModel fitting and validation (minimization of MSE)\n\nAssessing and predicting factors related to the spread of wildfires across Oregon is multifaceted, including geospatial and feature-related questions. Because of this, it will be crucial for us to utilise many of the packages and techniques discussed in class."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "Oregon Wildfire Prediction",
    "section": "Find out more",
    "text": "Find out more\nFor more information about US wide wildfires, see this link. For more information about Oregon wildfires, see this official website for Oregon Wildfire Risk and Recovery."
  },
  {
    "objectID": "analysis/Final.html",
    "href": "analysis/Final.html",
    "title": "Import Data and Libraries",
    "section": "",
    "text": "import datetime\nfrom jdcal import jd2gcal\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sqlite3\nimport xarray as xr\nimport geopandas as gpd\nfrom matplotlib import pyplot as plt\n\nimport cdsapi\nimport sklearn_pandas as skp\n\ncustom_colors = ['68A33E', '#A10702', '#FB9E60', '#FFFF82', '#0F0326']\n\n\nLoad in Raw Data\n\n##################################################\n##### 1. Loading raw data #####\ninput_filename = './Data/FPA_FOD_20170508.sqlite'\nconn = sqlite3.connect(input_filename)\nquery = '''\n    SELECT\n        NWCG_REPORTING_AGENCY, NWCG_REPORTING_UNIT_ID,\n        NWCG_REPORTING_UNIT_NAME,\n        FIRE_NAME,\n        COMPLEX_NAME,\n        FIRE_YEAR,\n        DISCOVERY_DATE,\n        DISCOVERY_DOY,\n        DISCOVERY_TIME,\n        CONT_DATE,\n        CONT_DOY,\n        CONT_TIME,\n        FIRE_SIZE,\n        FIRE_SIZE_CLASS,\n        OWNER_CODE,\n        OWNER_DESCR,\n        LATITUDE,\n        LONGITUDE,\n        STATE,\n        COUNTY\n    FROM\n        Fires;\n'''\ndf_raw = pd.read_sql_query(query, conn)\ndf_raw.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1880465 entries, 0 to 1880464\nData columns (total 20 columns):\n #   Column                    Dtype  \n---  ------                    -----  \n 0   NWCG_REPORTING_AGENCY     object \n 1   NWCG_REPORTING_UNIT_ID    object \n 2   NWCG_REPORTING_UNIT_NAME  object \n 3   FIRE_NAME                 object \n 4   COMPLEX_NAME              object \n 5   FIRE_YEAR                 int64  \n 6   DISCOVERY_DATE            float64\n 7   DISCOVERY_DOY             int64  \n 8   DISCOVERY_TIME            object \n 9   CONT_DATE                 float64\n 10  CONT_DOY                  float64\n 11  CONT_TIME                 object \n 12  FIRE_SIZE                 float64\n 13  FIRE_SIZE_CLASS           object \n 14  OWNER_CODE                float64\n 15  OWNER_DESCR               object \n 16  LATITUDE                  float64\n 17  LONGITUDE                 float64\n 18  STATE                     object \n 19  COUNTY                    object \ndtypes: float64(7), int64(2), object(11)\nmemory usage: 286.9+ MB\n\n\n\ninput_filename = './Data/FPA_FOD_20170508.sqlite'\nconn = sqlite3.connect(input_filename)\nquery = '''\n    SELECT\n       *\n    FROM\n        Fires;\n'''\ndf_raw = pd.read_sql_query(query, conn)\ndf_raw.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1880465 entries, 0 to 1880464\nData columns (total 39 columns):\n #   Column                      Dtype  \n---  ------                      -----  \n 0   OBJECTID                    int64  \n 1   FOD_ID                      int64  \n 2   FPA_ID                      object \n 3   SOURCE_SYSTEM_TYPE          object \n 4   SOURCE_SYSTEM               object \n 5   NWCG_REPORTING_AGENCY       object \n 6   NWCG_REPORTING_UNIT_ID      object \n 7   NWCG_REPORTING_UNIT_NAME    object \n 8   SOURCE_REPORTING_UNIT       object \n 9   SOURCE_REPORTING_UNIT_NAME  object \n 10  LOCAL_FIRE_REPORT_ID        object \n 11  LOCAL_INCIDENT_ID           object \n 12  FIRE_CODE                   object \n 13  FIRE_NAME                   object \n 14  ICS_209_INCIDENT_NUMBER     object \n 15  ICS_209_NAME                object \n 16  MTBS_ID                     object \n 17  MTBS_FIRE_NAME              object \n 18  COMPLEX_NAME                object \n 19  FIRE_YEAR                   int64  \n 20  DISCOVERY_DATE              float64\n 21  DISCOVERY_DOY               int64  \n 22  DISCOVERY_TIME              object \n 23  STAT_CAUSE_CODE             float64\n 24  STAT_CAUSE_DESCR            object \n 25  CONT_DATE                   float64\n 26  CONT_DOY                    float64\n 27  CONT_TIME                   object \n 28  FIRE_SIZE                   float64\n 29  FIRE_SIZE_CLASS             object \n 30  LATITUDE                    float64\n 31  LONGITUDE                   float64\n 32  OWNER_CODE                  float64\n 33  OWNER_DESCR                 object \n 34  STATE                       object \n 35  COUNTY                      object \n 36  FIPS_CODE                   object \n 37  FIPS_NAME                   object \n 38  Shape                       object \ndtypes: float64(8), int64(4), object(27)\nmemory usage: 559.5+ MB\n\n\n\n\nClean Data and Extract Oregon Boundaries\n\n##################################################\n##### 2. Cleaning data and extracting Oregon #####\ndrop_columns = ['NWCG_REPORTING_AGENCY',\n                'NWCG_REPORTING_UNIT_ID',\n                'NWCG_REPORTING_UNIT_NAME',\n                'FIRE_NAME',\n                'COMPLEX_NAME', \n                'OWNER_DESCR',\n                'FIRE_SIZE_CLASS',\n                'OWNER_CODE']\n#df_CA = df_raw[df_raw.STATE == 'CA'].drop(columns=drop_columns)\ndf_OR = df_raw[df_raw.STATE == 'OR'].drop(columns=drop_columns)\n#Extracting onset month and day\ndf_OR['MONTH'] = df_OR['DISCOVERY_DATE'].apply(lambda x: jd2gcal(x, 0)[1])\ndf_OR['DAY'] = df_OR['DISCOVERY_DATE'].apply(lambda x: jd2gcal(x, 0)[2])\ndf_OR.head(2)\n\nus_states = gpd.read_file(\"./Data/cb_2018_us_state_500k/cb_2018_us_state_500k.shp\")\noregon = us_states[us_states['NAME'] == 'Oregon']\noregon.plot(ax=plt.gca(), color='white', edgecolor='red')\nplt.title(\"Oregon Boundary\")\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\nMonthly/Seasonaly fire frequency\n\n\ncustom_colors = ['#68A33E','#FFFF82','#FB9E60','#A10702', '#0F0326']  # Add your desired hex colors\n\n#custom_cmap = ListedColormap(custom_colors)\n\n##################################################\n##### 3. Monthly/seasonal fire frequency #####\ndf_freq_mon = df_OR.groupby(['MONTH', 'FIRE_YEAR']).size().unstack()\n# plot monthly frequency of fire events\ncounter_fig = 1\nmon_ticks = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\nplt.figure(figsize=[11,5])\nsns.heatmap(df_freq_mon, cmap=custom_colors, linewidth=.2, linecolor=[.9,.9,.9])\nplt.yticks(np.arange(0.5,12.5), labels=mon_ticks, rotation=0, fontsize=12)\nplt.xticks(fontsize=12);\nplt.xlabel('')\nplt.ylabel('Month', fontsize=13)\nplt.title(f'Fig {counter_fig}. Number of fire events in Oregon', fontsize=13)\nplt.tight_layout()\n#plt.savefig(f'../Figures/Fig{counter_fig}.png', format='png', dpi=200)\n\n# plot fire frequency by cause and day of year\ncause_by_doy = df_OR.groupby(['STAT_CAUSE_DESCR','DISCOVERY_DOY']).size().unstack()\ncounter_fig +=1\nplt.figure(figsize=[10,5])\nax = sns.heatmap(cause_by_doy,cmap=custom_colors,vmin=0,vmax=500) #'CMRmap_r' &lt;- old color scheme\nplt.xticks(np.arange(0.5,366.5,20), labels=range(1,366,20), rotation=0, fontsize=11)\nplt.yticks(fontsize=11)\nplt.ylabel('Fire Cause', fontsize=12)\nplt.xlabel('Day of year', fontsize=12)\nfor borders in [\"top\",\"right\",\"left\",\"bottom\"]:\n    ax.spines[borders].set_visible(True)\nplt.title(f'Fig {counter_fig}. Distribution of OR fires by cause & day of year')\nplt.tight_layout()\n#plt.savefig(f'../Figures/Fig{counter_fig}.png', format='png', dpi=200)\n\n\n\n\n\n\n\n\n\n\nDownload and Extract Climate Data (ERAS)\n\nimport os\n\n# Get the user directory within the current environment\nenv_name = 'musa-550-fall-2023'\nenv_user_dir = os.path.join(os.environ['CONDA_PREFIX'], 'envs', env_name)\n\nprint(f\"User directory in {env_name} environment: {env_user_dir}\")\n\nUser directory in musa-550-fall-2023 environment: C:\\Users\\kathl\\mambaforge\\envs\\musa-550-fall-2023\\envs\\musa-550-fall-2023\n\n\n\nds_era5 = xr.open_dataset('ERA5_monthly.nc')\n\nImportError: DLL load failed while importing _netCDF4: The specified procedure could not be found.\n\n\n\n##################################################\n##### 4. Download and extract climate data (ERA5) #####\n\nimport cdsapi\n\ndef _download_era5(variables: list, filename: str):\n    c = cdsapi.Client()\n    c.retrieve(\n        'reanalysis-era5-single-levels-monthly-means',\n        {\n            'format': 'netcdf',\n            'product_type': 'monthly_averaged_reanalysis',\n            'variable': variables,\n            'year': [f'{yr}' for yr in range(1997)],  # Adjust the range of years as needed\n            'month': [f'{x:02.0f}' for x in range(3, 10)],  # Include all months\n            'time': '00:00',\n            'area': [46.3, -124.6, 42.4, -122.9],  # Oregon coordinates\n        },\n        f'{filename}.nc'\n    )\n\nvars_all = ['total_precipitation', '2m_temperature', '2m_dewpoint_temperature',\n            '10m_wind_speed', 'volumetric_soil_water_layer_1', 'potential_evaporation']\n\n_download_era5(vars_all, 'ERA5_monthly')\n\n#ds_era5 = xr.open_dataset('ERA5_monthly.nc')\n# load Oregon mask file\nOR_mask = oregon\n\n2023-12-20 20:17:54,620 INFO Welcome to the CDS\n2023-12-20 20:17:54,621 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/reanalysis-era5-single-levels-monthly-means\n2023-12-20 20:17:55,282 INFO Request is queued\n2023-12-20 20:17:58,256 INFO Request is failed\n2023-12-20 20:17:58,256 ERROR Message: an internal error occurred processing your request\n2023-12-20 20:17:58,256 ERROR Reason:  day is out of range for month: 31-04-01\n2023-12-20 20:17:58,264 ERROR   Traceback (most recent call last):\n2023-12-20 20:17:58,266 ERROR     File \"/usr/local/lib/python3.6/site-packages/dateutil/parser/_parser.py\", line 1235, in _build_naive\n2023-12-20 20:17:58,269 ERROR       naive = default.replace(**repl)\n2023-12-20 20:17:58,269 ERROR   ValueError: day is out of range for month\n\n\nException: an internal error occurred processing your request. day is out of range for month: 31-04-01.\n\n\n\n\nAssessing climate trends\n\n##################################################\n##### 5. Assessing climate trends #####\n# plot climate trends\n#ds_era5 = \"./'ERA5_monthly.nc'\"\n\n\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n# Load data from the NetCDF file into ds_era5\nds_era5 = xr.open_dataset('ERA5_monthly.nc')  # Replace 'ERA5_monthly.nc' with your actual file path\n\ncounter_fig = 0\ncounter_fig += 1\nvar = list(ds_era5.data_vars)\n# Rest of the code remains the same...\n\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n# Assuming ds_era5 is an xarray dataset loaded from the NetCDF file\ncounter_fig = 0\ncounter_fig +=1\nvar = list(ds_era5.data_vars)\ntitles = {\"tp\":\"Precipitation (mm)\",\n         \"t2m\":\"Air temperature at 2m (°C)\",\n         \"d2m\":\"Dew point temperature (°C)\",\n         \"si10\":\"Wind speed at 10m\",\n         \"swvl1\":\"Near surface soil moisture\",\n         \"pev\":\"Potential evapotranspiration (mm)\"}\nunit_conversion = {\"tp\": [1000*365,0], #convert default units to mm and °C --&gt;[*multiply, *add]\n                   \"t2m\": [1,-273.15], \n                   \"d2m\": [1,-273.15],\n                   \"si10\": [1,0],\n                   \"swvl1\": [1,0],\n                   \"pev\": [-1000*365,0]}\nfig = plt.figure(figsize=(13, 5.5))\ngs = gridspec.GridSpec(2, 3) \nfor i in range(len(var)):\n    plt.subplot(gs[i])\n    (ds_era5[var[i]].where(OR_mask).mean(['latitude','longitude']).groupby(\n        ds_era5.time.dt.year).mean()*unit_conversion[var[i]][0] + \\\n        unit_conversion[var[i]][1]).plot(color='g')\n    plt.xlabel('')\n    plt.grid(axis='y')\n    plt.title(titles[var[i]], fontsize=11, fontweight='bold')\nplt.tight_layout()\nplt.savefig(f'../Figures/Fig{counter_fig}.png', format='png', dpi=200)\n\nImportError: DLL load failed while importing _netCDF4: The specified procedure could not be found.\n\n\n\n\nAssessing the climate factors affecting fires\n\n##################################################\n##### 6. Assessing the climate factors affecting fires #####\ndf_freq = pd.DataFrame(df_OR.groupby(['FIRE_YEAR','MONTH']).size(), columns=['Frequency'])\ndf_freq = df_freq.reset_index().rename(columns={'FIRE_YEAR':'Year', 'MONTH':'Month'})\n# extract monthly ERA5 data\ndef extract_era5_yr_mon(yr, mon, var):\n    unit_conversion = {\"tp\": [1000*30,0], #convert default units to mm and °C --&gt;[*multiply, +add]\n                       \"t2m\": [1,-273.15], \n                       \"d2m\": [1,-273.15],\n                       \"si10\": [1,0],\n                       \"swvl1\": [1,0],\n                       \"pev\": [-1000*30,0]}\n    ds_yr_mon = ds_era5.sel(time = ds_era5.time.dt.month.isin(mon) & ds_era5.time.dt.year.isin(yr))\n    output = float(ds_yr_mon[var].where(OR_mask).mean(['latitude','longitude']).values) * \\\n    unit_conversion[var][0] + unit_conversion[var][1]\n    return output\n\n# adding climate data to the dataframe\nfor i in range(len(var)):\n    df_freq[var[i]] = df_freq.apply(lambda x: extract_era5_yr_mon(x.Year, x.Month, var[i]), axis=1)\n\n# plotting scatter matrix for summer (MJJAS)\nplt.style.use('seaborn')\ncounter_fig +=1\ndf_freq_summer = df_freq[df_freq.Month.isin([5,6,7,8,9])]\npd.plotting.scatter_matrix(df_freq_summer.drop(\n    columns=['Year','Month']), alpha=0.5, color='g', figsize=(11, 11), diagonal='kde',);\nplt.tight_layout()\nplt.savefig(f'Fig{counter_fig}.png', format='png', dpi=200)\n\n# calculate correlation between fire frequency and climate variables\ndef calculate_R2(array1, array2):\n    return np.corrcoef(array1,array2)[0,1]\n\ndf_corr=np.zeros(len(var))\nfor i in range(len(var)):\n    df_corr[i] = calculate_R2(df_freq_summer['Frequency'], df_freq_summer[var[i]])\n    \n# plot correlation results\ncounter_fig +=1\nfig = plt.figure(figsize=(6, 6))\nsns.barplot(x=var, y=df_corr, palette='Spectral')\nplt.plot([-1,6],[0,0],':k',linewidth=1.3)\nplt.xticks(ticks=range(6), labels=['Precipitation', 'Air temperature', 'Dew point temperature',\n                                  'Wind speed', 'Soil moisture', 'Potential Evapotranspiration'],\n          fontsize=11, rotation=-90)\nplt.xlim([-.5,5.5])\nplt.yticks(fontsize=11)\nplt.ylabel('Correlation Coefficient', fontsize=11.5)\nplt.tight_layout()\nplt.savefig(f'Fig{counter_fig}.png', format='png', dpi=200)\n\n\n\n\nSetup the ANN model\n\n##################################################\n##### 7. Setup the ANN model #####\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndef ANN_fit(X,y):\n    # temporal split into train and test\n    ind_split = int(len(y)*.7)\n    X_train = X[:ind_split];    X_test = X[ind_split:]\n    y_train = y[:ind_split];    y_test = y[ind_split:]\n    \n    # scaling data\n    scaler = MinMaxScaler()\n    scaler.fit(X_train)\n    X_train = scaler.transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    # setting up and fitting the model\n    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n    model = Sequential()\n    model.add(Dense(16,activation='relu'))\n    model.add(Dense(16,activation='relu'))\n    model.add(Dense(8,activation='relu'))\n    #model.add(Dense(4,activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse')\n    history = model.fit(X_train, y_train, epochs=3500, validation_data=(X_test, y_test), \n                        verbose=0, shuffle=False, callbacks=[early_stop])\n    \n    # plotting the loss function\n    plt.plot(history.history['loss'], label='train')\n    plt.plot(history.history['val_loss'], label='test')\n    plt.legend()\n    plt.show()\n    \n    # predicting the test period \n    test_predictions = model.predict(X_test)\n    test_predictions = pd.Series(test_predictions.reshape(test_predictions.shape[0], )) \n    pred_df = pd.DataFrame(y_test, columns=['Test True Y']) \n    pred_df = pd.concat([pred_df, test_predictions], axis=1)\n    pred_df.columns = ['Test True Y', 'Model Prediction']\n    pred_df['Year'] = df_freq[ind_split:].Year.values\n    pred_df['Month'] = df_freq[ind_split:].Month.values\n\n    return pred_df\n\n\n\nFitting the ANN Model\n\n##################################################\n##### 8. Fitting the ANN model #####\nplt.style.use('ggplot')\ny = df_freq['Frequency'].values\nX = df_freq.drop(columns=['Frequency']).values\npred_df = ANN_fit(X, y)\n\n# plotting the prediction vs. observation\ncounter_fig +=1\nplt.figure(figsize=[11,5])\nplt.plot(pred_df['Test True Y'], color='g', linewidth=2, label='Truth')\nplt.plot(pred_df['Model Prediction'], color='r', linewidth=2, label='Prediction')\nplt.xticks(ticks=range(0,len(pred_df),12), labels=pred_df.Year[::12])\nplt.ylim(bottom=0)\nplt.ylabel('Number of fire events', fontsize=12)\nplt.legend(fontsize=12)\nplt.tight_layout()\nplt.savefig(f'Fig{counter_fig+1}.png', format='png', dpi=200)"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html",
    "href": "analysis/3-altair-hvplot.html",
    "title": "Altair and Hvplot Charts",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive charts produced using Altair and hvPlot."
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-altair",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in Altair",
    "text": "Example: Measles Incidence in Altair\nFirst, let’s load the data for measles incidence in wide format:\n\n\nCode\nurl = \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/measles_incidence.csv\"\ndata = pd.read_csv(url, skiprows=2, na_values=\"-\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nWEEK\nALABAMA\nALASKA\nARIZONA\nARKANSAS\nCALIFORNIA\nCOLORADO\nCONNECTICUT\nDELAWARE\n...\nSOUTH DAKOTA\nTENNESSEE\nTEXAS\nUTAH\nVERMONT\nVIRGINIA\nWASHINGTON\nWEST VIRGINIA\nWISCONSIN\nWYOMING\n\n\n\n\n0\n1928\n1\n3.67\nNaN\n1.90\n4.11\n1.38\n8.38\n4.50\n8.58\n...\n5.69\n22.03\n1.18\n0.4\n0.28\nNaN\n14.83\n3.36\n1.54\n0.91\n\n\n1\n1928\n2\n6.25\nNaN\n6.40\n9.91\n1.80\n6.02\n9.00\n7.30\n...\n6.57\n16.96\n0.63\nNaN\n0.56\nNaN\n17.34\n4.19\n0.96\nNaN\n\n\n2\n1928\n3\n7.95\nNaN\n4.50\n11.15\n1.31\n2.86\n8.81\n15.88\n...\n2.04\n24.66\n0.62\n0.2\n1.12\nNaN\n15.67\n4.19\n4.79\n1.36\n\n\n3\n1928\n4\n12.58\nNaN\n1.90\n13.75\n1.87\n13.71\n10.40\n4.29\n...\n2.19\n18.86\n0.37\n0.2\n6.70\nNaN\n12.77\n4.66\n1.64\n3.64\n\n\n4\n1928\n5\n8.03\nNaN\n0.47\n20.79\n2.38\n5.13\n16.80\n5.58\n...\n3.94\n20.05\n1.57\n0.4\n6.70\nNaN\n18.83\n7.37\n2.91\n0.91\n\n\n\n\n5 rows × 53 columns\n\n\n\nThen, use the pandas.melt() function to convert it to tidy format:\n\n\nCode\nannual = data.drop(\"WEEK\", axis=1)\nmeasles = annual.groupby(\"YEAR\").sum().reset_index()\nmeasles = measles.melt(id_vars=\"YEAR\", var_name=\"state\", value_name=\"incidence\")\n\n\n\n\n\n\n\n\n\n\n\nYEAR\nstate\nincidence\n\n\n\n\n0\n1928\nALABAMA\n334.99\n\n\n1\n1929\nALABAMA\n111.93\n\n\n2\n1930\nALABAMA\n157.00\n\n\n3\n1931\nALABAMA\n337.29\n\n\n4\n1932\nALABAMA\n10.21\n\n\n\n\n\n\n\nFinally, load altair:\n\nimport altair as alt\n\nAnd generate our final data viz:\n\n# use a custom color map\ncolormap = alt.Scale(\n    domain=[0, 100, 200, 300, 1000, 3000],\n    range=[\n        \"#F0F8FF\",\n        \"cornflowerblue\",\n        \"mediumseagreen\",\n        \"#FFEE00\",\n        \"darkorange\",\n        \"firebrick\",\n    ],\n    type=\"sqrt\",\n)\n\n# Vertical line for vaccination year\nthreshold = pd.DataFrame([{\"threshold\": 1963}])\n\n# plot YEAR vs state, colored by incidence\nchart = (\n    alt.Chart(measles)\n    .mark_rect()\n    .encode(\n        x=alt.X(\"YEAR:O\", axis=alt.Axis(title=None, ticks=False)),\n        y=alt.Y(\"state:N\", axis=alt.Axis(title=None, ticks=False)),\n        color=alt.Color(\"incidence:Q\", sort=\"ascending\", scale=colormap, legend=None),\n        tooltip=[\"state\", \"YEAR\", \"incidence\"],\n    )\n    .properties(width=650, height=500)\n)\n\nrule = alt.Chart(threshold).mark_rule(strokeWidth=4).encode(x=\"threshold:O\")\n\nout = chart + rule\nout"
  },
  {
    "objectID": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "href": "analysis/3-altair-hvplot.html#example-measles-incidence-in-hvplot",
    "title": "Altair and Hvplot Charts",
    "section": "Example: Measles Incidence in hvplot",
    "text": "Example: Measles Incidence in hvplot\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate the same data viz in hvplot:\n\n# Make the heatmap with hvplot\nheatmap = measles.hvplot.heatmap(\n    x=\"YEAR\",\n    y=\"state\",\n    C=\"incidence\", # color each square by the incidence\n    reduce_function=np.sum, # sum the incidence for each state/year\n    frame_height=450,\n    frame_width=600,\n    flip_yaxis=True,\n    rot=90,\n    colorbar=False,\n    cmap=\"viridis\",\n    xlabel=\"\",\n    ylabel=\"\",\n)\n\n# Some additional formatting using holoviews \n# For more info: http://holoviews.org/user_guide/Customizing_Plots.html\nheatmap = heatmap.redim(state=\"State\", YEAR=\"Year\")\nheatmap = heatmap.opts(fontsize={\"xticks\": 0, \"yticks\": 6}, toolbar=\"above\")\nheatmap"
  },
  {
    "objectID": "analysis/1-python-code-blocks.html",
    "href": "analysis/1-python-code-blocks.html",
    "title": "Python code blocks",
    "section": "",
    "text": "This is an example from the Quarto documentation that shows how to mix executable Python code blocks into a markdown file in a “Quarto markdown” .qmd file.\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "We are Kathleen and Shreya. Our collaboration goes beyond this project; we have been roommates since semester 1 and have wanted to work on an interesting project together. Based on the incessant conversations in the living room until 2am, we really wanted to study and visualize an impending problem and test whether it is affected by climate change."
  },
  {
    "objectID": "analysis/2-static-images.html",
    "href": "analysis/2-static-images.html",
    "title": "Showing static visualizations",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and demonstrates how to generate static visualizations with matplotlib, pandas, and seaborn.\nStart by importing the packages we need:\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nLoad the “Palmer penguins” dataset from week 2:\n# Load data on Palmer penguins\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-2/main/data/penguins.csv\")\n# Show the first ten rows\npenguins.head(n=10)    \n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nmale\n2007\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nfemale\n2007\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nmale\n2007\n\n\n8\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNaN\n2007\n\n\n9\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNaN\n2007"
  },
  {
    "objectID": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "href": "analysis/2-static-images.html#a-simple-visualization-3-different-ways",
    "title": "Showing static visualizations",
    "section": "A simple visualization, 3 different ways",
    "text": "A simple visualization, 3 different ways\n\nI want to scatter flipper length vs. bill length, colored by the penguin species\n\n\nUsing matplotlib\n\n# Setup a dict to hold colors for each species\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Initialize the figure \"fig\" and axes \"ax\"\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Group the data frame by species and loop over each group\n# NOTE: \"group\" will be the dataframe holding the data for \"species\"\nfor species, group_df in penguins.groupby(\"species\"):\n\n    # Plot flipper length vs bill length for this group\n    # Note: we are adding this plot to the existing \"ax\" object\n    ax.scatter(\n        group_df[\"flipper_length_mm\"],\n        group_df[\"bill_length_mm\"],\n        marker=\"o\",\n        label=species,\n        color=color_map[species],\n        alpha=0.75,\n        zorder=10\n    )\n\n# Plotting is done...format the axes!\n\n## Add a legend to the axes\nax.legend(loc=\"best\")\n\n## Add x-axis and y-axis labels\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\n\n## Add the grid of lines\nax.grid(True);\n\n\n\n\n\n\nHow about in pandas?\nDataFrames have a built-in “plot” function that can make all of the basic type of matplotlib plots!\nFirst, we need to add a new “color” column specifying the color to use for each species type.\nUse the pd.replace() function: it use a dict to replace values in a DataFrame column.\n\n# Calculate a list of colors\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\n\n# Map species name to color \npenguins[\"color\"] = penguins[\"species\"].replace(color_map)\n\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\ncolor\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n#1f77b4\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n#1f77b4\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n#1f77b4\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n#1f77b4\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n#1f77b4\n\n\n\n\n\n\n\nNow plot!\n\n# Same as before: Start by initializing the figure and axes\nfig, myAxes = plt.subplots(figsize=(10, 6))\n\n# Scatter plot two columns, colored by third\n# Use the built-in pandas plot.scatter function\npenguins.plot.scatter(\n    x=\"flipper_length_mm\",\n    y=\"bill_length_mm\",\n    c=\"color\",\n    alpha=0.75,\n    ax=myAxes, # IMPORTANT: Make sure to plot on the axes object we created already!\n    zorder=10\n)\n\n# Format the axes finally\nmyAxes.set_xlabel(\"Flipper Length (mm)\")\nmyAxes.set_ylabel(\"Bill Length (mm)\")\nmyAxes.grid(True);\n\n\n\n\nNote: no easy way to get legend added to the plot in this case…\n\n\nSeaborn: statistical data visualization\nSeaborn is designed to plot two columns colored by a third column…\n\n# Initialize the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# style keywords as dict\ncolor_map = {\"Adelie\": \"#1f77b4\", \"Gentoo\": \"#ff7f0e\", \"Chinstrap\": \"#D62728\"}\nstyle = dict(palette=color_map, s=60, edgecolor=\"none\", alpha=0.75, zorder=10)\n\n# use the scatterplot() function\nsns.scatterplot(\n    x=\"flipper_length_mm\",  # the x column\n    y=\"bill_length_mm\",  # the y column\n    hue=\"species\",  # the third dimension (color)\n    data=penguins,  # pass in the data\n    ax=ax,  # plot on the axes object we made\n    **style  # add our style keywords\n)\n\n# Format with matplotlib commands\nax.set_xlabel(\"Flipper Length (mm)\")\nax.set_ylabel(\"Bill Length (mm)\")\nax.grid(True)\nax.legend(loc=\"best\");"
  },
  {
    "objectID": "analysis/4-folium.html",
    "href": "analysis/4-folium.html",
    "title": "Interactive Maps with Folium",
    "section": "",
    "text": "This page is generated from a Jupyter notebook and shows examples of embedding interactive maps produced using Folium."
  },
  {
    "objectID": "analysis/4-folium.html#finding-the-shortest-route",
    "href": "analysis/4-folium.html#finding-the-shortest-route",
    "title": "Interactive Maps with Folium",
    "section": "Finding the shortest route",
    "text": "Finding the shortest route\nThis example finds the shortest route between the Art Musuem and the Liberty Bell using osmnx.\n\nimport osmnx as ox\n\nFirst, identify the lat/lng coordinates for our places of interest. Use osmnx to download the geometries for the Libery Bell and Art Museum.\n\nphilly_tourism = ox.features_from_place(\"Philadelphia, PA\", tags={\"tourism\": True})\n\n\nart_museum = philly_tourism.query(\"name == 'Philadelphia Museum of Art'\").squeeze()\n\nart_museum.geometry\n\n\n\n\n\nliberty_bell = philly_tourism.query(\"name == 'Liberty Bell'\").squeeze()\n\nliberty_bell.geometry\n\n\n\n\nNow, extract the lat and lng coordinates\nFor the Art Museum geometry, we can use the .geometry.centroid attribute to calculate the centroid of the building footprint.\n\nliberty_bell_x = liberty_bell.geometry.x\nliberty_bell_y = liberty_bell.geometry.y\n\n\nart_museum_x = art_museum.geometry.centroid.x\nart_museum_y = art_museum.geometry.centroid.y\n\nNext, use osmnx to download the street graph around Center City.\n\nG_cc = ox.graph_from_address(\n    \"City Hall, Philadelphia, USA\", dist=1500, network_type=\"drive\"\n)\n\nNext, identify the nodes in the graph closest to our points of interest.\n\n# Get the origin node (Liberty Bell)\norig_node = ox.nearest_nodes(G_cc, liberty_bell_x, liberty_bell_y)\n\n# Get the destination node (Art Musuem)\ndest_node = ox.nearest_nodes(G_cc, art_museum_x, art_museum_y)\n\nFind the shortest path, based on the distance of the edges:\n\n# Get the shortest path --&gt; just a list of node IDs\nroute = ox.shortest_path(G_cc, orig_node, dest_node, weight=\"length\")\n\nHow about an interactive version?\nosmnx has a helper function ox.utils_graph.route_to_gdf() to convert a route to a GeoDataFrame of edges.\n\nox.utils_graph.route_to_gdf(G_cc, route, weight=\"length\").explore(\n    tiles=\"cartodb positron\",\n    color=\"red\",\n)\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/4-folium.html#examining-trash-related-311-requests",
    "href": "analysis/4-folium.html#examining-trash-related-311-requests",
    "title": "Interactive Maps with Folium",
    "section": "Examining Trash-Related 311 Requests",
    "text": "Examining Trash-Related 311 Requests\nFirst, let’s load the dataset from a CSV file and convert to a GeoDataFrame:\n\n\nCode\n# Load the data from a CSV file into a pandas DataFrame\ntrash_requests_df = pd.read_csv(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/trash_311_requests_2020.csv\"\n)\n\n# Remove rows with missing geometry\ntrash_requests_df = trash_requests_df.dropna(subset=[\"lat\", \"lon\"])\n\n\n# Create our GeoDataFrame with geometry column created from lon/lat\ntrash_requests = gpd.GeoDataFrame(\n    trash_requests_df,\n    geometry=gpd.points_from_xy(trash_requests_df[\"lon\"], trash_requests_df[\"lat\"]),\n    crs=\"EPSG:4326\",\n)\n\n\nLoad neighborhoods and do the spatial join to associate a neighborhood with each ticket:\n\n\nCode\n# Load the neighborhoods\nneighborhoods = gpd.read_file(\n    \"https://raw.githubusercontent.com/MUSA-550-Fall-2023/week-4/main/data/zillow_neighborhoods.geojson\"\n)\n\n# Do the spatial join to add the \"ZillowName\" column\nrequests_with_hood = gpd.sjoin(\n    trash_requests,\n    neighborhoods.to_crs(trash_requests.crs),\n    predicate=\"within\",\n)\n\n\nLet’s explore the 311 requests in the Greenwich neighborhood of the city:\n\n# Extract out the point tickets for Greenwich\ngreenwich_tickets = requests_with_hood.query(\"ZillowName == 'Greenwich'\")\n\n\n# Get the neighborhood boundary for Greenwich\ngreenwich_geo = neighborhoods.query(\"ZillowName == 'Greenwich'\")\n\ngreenwich_geo.squeeze().geometry\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nQuarto has callout blocks that you can use to emphasize content in different ways. This is a “Note” callout block. More info is available on the Quarto documentation.\n\n\nImport the packages we need:\n\nimport folium\nimport xyzservices\n\nCombine the tickets as markers and the neighborhood boundary on the same Folium map:\n\n# Plot the neighborhood boundary\nm = greenwich_geo.explore(\n    style_kwds={\"weight\": 4, \"color\": \"black\", \"fillColor\": \"none\"},\n    name=\"Neighborhood boundary\",\n    tiles=xyzservices.providers.CartoDB.Voyager,\n)\n\n\n# Add the individual tickets as circle markers and style them\ngreenwich_tickets.explore(\n    m=m,  # Add to the existing map!\n    marker_kwds={\"radius\": 7, \"fill\": True, \"color\": \"crimson\"},\n    marker_type=\"circle_marker\", # or 'marker' or 'circle'\n    name=\"Tickets\",\n)\n\n# Hse folium to add layer control\nfolium.LayerControl().add_to(m)\n\nm  # show map\n\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "analysis/index.html",
    "href": "analysis/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  }
]